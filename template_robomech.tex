\documentclass{jarticle}
% \usepackage{flushend}  % 最終ページの2カラムの左右の高さを揃える
\usepackage[dvipdfmx]{graphicx}
\usepackage{subcaption}
\captionsetup[figure]{justification=centering}
\captionsetup[table]{justification=centering}
\usepackage[ipa]{pxchfon}
\usepackage{otf}
\renewcommand{\figurename}{図~}
\renewcommand{\tablename}{表~}
\newcommand{\figref}[1]{\figurename\ref{#1}}
\newcommand{\tabref}[1]{\tablename\ref{#1}}
\usepackage{robomech}
% \usepackage[dvipdfmx,setpagesize=false]{hyperref}



\begin{document}
\makeatletter
\title{視覚と行動のend-to-end学習により\\経路追従行動をオンラインで模倣する手法の提案}
{― データセット収集密度の動的調整による学習の効率化 ―}
{A proposal for an online imitation method of
path-tracking\\ behavior by end-to-end learning of vision and action}
{- Efficiency improvement of learning by dynamic adjustment of dataset collection density -}

\author{
\begin{tabular}{c}
 \hspace*{0.3zw}○学\hspace{1zw}今井悠月 (千葉工大)\hspace{2zw}正\hspace{1zw}上田隆一 (千葉工大)\hspace{2zw}正\hspace{1zw}林原靖男(千葉工大)\\
 \end{tabular}
 \vspace{1zh} \\
 \begin{tabular}{l}
 {\hspace*{4zw}\small Yuzuki IMAI, Chiba Institute of Technology, s20c1015as@s.chibakoudai.jp}\\
 {\hspace*{4zw}\small Ryuichi UEDA, Chiba Institute of Technology}\\
 {\hspace*{4zw}\small Yasuo HAYASHIBARA, Chiba Institute of Technology}\\
\end{tabular}
}
\makeatother

\abstract{ \small 
We have proposed an online imitation method of path-tracking behavior based on end-to-end learning of vision and action.
In recent years, many studies of autonomous movement using end-to-end learning have been reported. However, these studies have also observed deviations from the target path.
One of the possible reasons for this is the lack of training data for returning to the path.
In this paper, we perform end-to-end learning to follow a route generated by a map-based navigation system. The dataset was collected in two ways, one is to learn only the area around the route and the other is to learn the state away from the route, and the generated path-tracking behaviors were analyzed.
In addition, we proposed a new method of collecting teacher data to reinforce the behavior of returning to the path, and verified the effectiveness of the method by experiments using a simulator.
}

\date{} % 日付を出力しない
\keywords{Autonomous mobile robot,  Navigation,  End-to-end learning,  Dataset}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\small
\section{緒言}
本研究グループは，移動ロボットにおけるナビゲーション手段の複数化を目標としている．その目標を達成するため，
従来からカメラ画像とロボットの角速度を end-to-end 学習することで，経路追従する手法を提案し，その有効性を
検証してきた\cite{okada}\cite{okada2}\cite{kiyooka}．
本手法は，複数のセンサと地図を入力として生成した行動を，画像を入力とする行動に模倣する．これによりロボット
は，複数のセンサと地図を使用した時と同じ行動を，画像のみで行えるようになる．よって，地図に基づく経路追従と
画像に基づく経路追従の 2 つのナビゲーション手段を獲得できる．これらを状況に応じて高い信頼性が見込まれる方
を選択することで，経路追従を継続できる可能性が高まる．\\
\hspace*{1zw}本研究と同様に，カメラ画像を入力として，end-to-end 学習により経路追従する手法は，いくつか
提案されている．
例えば，Muller らは，人のコントローラ操作と画像を end-to-end 学習することで，オフロード環境で障害物を
回避して走行できることを確認した\cite{off_load}．また，Moridian らは，人のコントローラ操作と画像，測域
センサのデータを end-to-end 学習することで，一定の経路を走行できることを確認した\cite{Moridian}．
さらに，Bojarski らは，画像と人が操作したステアリングの角度を end-to-end 学習することで，自動運転する
手法を提案した\cite{Bojarski}．
これらの手法はすべて人の操作を教師データとしているのに対し，本手法はルールベースの制御器の出力である
角速度を教師データとしている．そのため，データセットの収集に人の操作が不要である．
それに加えて，ロボットが目標経路に復帰する行動を収集できるため，経路追従の継続に有効であることが確認
されている\cite{imai}．\\
\hspace*{1zw}ここで，本手法はロボットを走行させながらデータセットを収集し，それを用いて学習するが，
ロボットの移動速度に関してはこれまで議論されていない．
従来は，常に一定の速度（0.2 m/s）でロボットを走行させていたが，経路の一部で必要以上にデータを収集している
おそれがある．
収集したデータセットの順序は，各ステップ（0.2 s 程度）ごとに無作為に並び替えられ，その中の一部を学習に
用いるため，割合の大きいデータは多く学習され，割合の小さいデータはあまり学習されていない可能性がある．
経路の一部でも学習が不足すると，ロボットは経路追従できないため，データの収集密度を改善し，
学習に用いられるデータの偏りを少なくすることが望まれる．\\
\hspace*{1zw}これを踏まえ，本稿では，まず従来手法における経路追従失敗の一例を取り上げ，
その要因がデータセットの不均衡によるものであるかを明らかにする．
そのうえで，ロボットの移動速度を教師データの値に応じて可変にすることで，データセットの
収集密度を調整し，学習を効率化する手法を提案する．
なお，このような学習を効率化する手法は，これまでに本研究グループでもいくつか提案されている．
例えば，藤原らは，オーバーサンプリングによりデータの不均衡を改善する手法を提案した\cite{fuji}．
また，\CID{8705}橋らは，データを事前に収集し，オフラインで学習する手法を提案した\cite{takahashi}．
さらに，今井らは，モデルの評価に基づき訓練を打ち切る手法を提案した\cite{imai2}．
いずれの手法も訓練時間の短縮に成功しているが，本稿における提案手法とはアプローチが異なる．\\
\hspace*{1zw}本稿では，今井らの手法を採用し，シミュレータで実験する．
ロボットの移動速度を可変にする場合と，そうでない場合で結果を比較することにより，
提案手法の有効性を検証する．\\


\section{end-to-end学習により経路追従行動を模倣する手法}
本研究グループが従来から提案する手法について述べる．
本手法は，\figref{fig:1}に示すように，
64×48にリサイズした画像を入力，ロボットの角速度を出力として， end-to-end で学習する．
また，ネットワークの構造を \figref{fig:2} に示す．
左から順に，入力層 1 ，畳み込み層 3 ，全結合層 2 ，出力層 1 の全 7 層で構成されている．
なお，本手法では一定時間（0.2 s 程度）ごとにデータセットを収集しつつ，学習する．
これを 1 ステップとしており，1 ステップにつき，画像と角速度をペアにしたデータセットの中から
8 個のデータを抽出し，それ用いて学習する．

\begin{figure}[h!]
  \centering
   \includegraphics[height=20mm]{./pdf/deplearning.pdf}
   \caption{The end-to-end learning method}
   \label{fig:1}
\end{figure}

\begin{figure}[h!]
  \centering
   \includegraphics[height=43mm]{pdf/network.pdf}
   \caption{Structure of network}
   \label{fig:2}
\end{figure}


\subsection{地図に基づく経路追従の模倣学習}
地図ベースの経路追従を模倣学習する手法を\figref{fig:3.1}に示す．
本手法では, ルールベースの制御器による地図ベースの経路追従を利用することで，ロボットは自律移動を行う．
それと同時に，カメラ画像と ROS のnavigation パッケージから出力される
ヨー方向の目標角速度をペアにしたデータセットを収集し，end-to-end で学習する．
地図ベースの経路追従は LiDAR とオドメトリを入力とするのに対し，学習器は画像のみを入力とする．
本手法を用いることで，地図ベースの経路追従と同じ行動を，画像のみで行うことが可能である．
これによって，地図ベースの経路追従と画像ベースの経路追従の 2 つのナビゲーション手段が
得られる．この 2 つのナビゲーション手段を，状況に応じて高い信頼性が見込まれる方を選択することで，
ロボットが経路追従行動を継続できる可能性が高まると考えられる．\\
なお，3台のカメラ（左・中央・右）を用いて学習を行うが，この時， 
Table \ref{table:3.1}に示すように左右のカメラにオフセットを加える．
具体的には，カメラの角度を考慮し，それに合わせて角速度を変化させてデータセットに加えることで
データの量を増やすことや，過学習を防ぐ効果が期待される．
また，カメラ画像は，64×48 にリサイズしたものを使用する．\\
\hspace*{1zw}ロボットのモータを制御するモジュールは，ROS の navigation パッケージから出力される
角速度と学習器から出力される角速度の2つで構成されており，この2つを切り替えることが可能となっている．

\begin{figure}[h!]
  \centering
   \includegraphics[height=50mm]{./png/learn.png}
   \caption{The end-to-end learning method}
  %  \label{fig:1}
\end{figure}


\begin{table}[h!]
  \centering
  \caption{Yaw angular velocity offset per camera}
  \label{table:3.1}
    \scalebox{0.95}[0.95]{
    \begin{tabular}{|l|l|}
      \hline\hline
      Cameras & Yaw angular velocity offset (rad/s)\\
      \hline\hline
      Left camera & ROS navigation system output $-$ 0.2\\
      \hline
      Center camera & ROS navigation system output\\
      \hline
      Right camera & ROS navigation system output $+$ 0.2\\
      \hline
    \end{tabular} }
\end {table}




\subsection{訓練済みモデルを用いた視覚に基づく経路追従}
学習が終了したら，学習済みモデルを使用して，経路を走行できるかを確認する．
学習済みモデルを用いた視覚に基づく経路追従では，\figref{fig:3.2} のように，占有格子地図と LiDARなどの
センサや，オドメトリを入力とした ROS の navigation パッケージからの出力は使用せずにロボットを制御する．
地図ベースの経路追従の模倣学習時と同様に，64×48 にリサイズした画像を学習器に入力し，その
推定結果をロボットのヨー方向の目標角速度としてロボットを自律移動させる．
ここで，ロボットのヨー方向の角速度には学習器の出力を用いるが，並進速度は一定 (0.2m/s) とする．\\
\hspace*{1zw}なお，地図ベースの経路追従の模倣学習時には，3台のカメラ（左・中央・右）を用いたが，
学習済みモデルを用いた視覚に基づく経路追従時では，3台のカメラのうち，中央のカメラのみを使用する．\\

\begin{figure}[h!]
  \centering
   \includegraphics[height=50mm]{./png/afterlearn.png}
   \caption{The end-to-end learning method}
  %  \label{fig:2}
\end{figure}

\section{モデルの評価に基づき訓練を打ち切る手法}
あああ\\

\begin{figure}[h!]
  \centering
   \includegraphics[height=48mm]{./png/moderu.png}
   \caption{The end-to-end learning method}
  %  \label{fig:2}
\end{figure}

\newpage

\section{従来手法を用いた経路追従失敗の要因調査}
あああ



\begin{figure}[h!]
  \centering
   \includegraphics[height=46mm]{./png/location.png}
   \caption{The end-to-end learning method}
  %  \label{fig:2}
\end{figure}


\begin{figure}[h!]
  \centering
   \includegraphics[height=43mm]{./png/failed.png}
   \caption{The end-to-end learning method}
  %  \label{fig:2}
\end{figure}

\begin{figure}[h!]
  \centering
   \includegraphics[height=43mm]{./png/analy.png}
   \caption{The end-to-end learning method}
  %  \label{fig:2}
\end{figure}


\section{提案手法}
あああ\\

\newpage

\section{シミュレータでの実験}
あああ\\

\section{結言}
ああああ

\footnotesize
\begin{thebibliography}{99}

\bibitem{okada}
岡田眞也， 清岡優祐， 上田隆一， 林原靖男，“視覚と行動の end-
to-end 学習により経路追従行動をオンラインで模倣する手法
の提案”，計測自動制御学会 \textit{SI} 部門講演会 \textit{SICE-SI2020} 予稿
集， pp.1148-1152, 2020.

\bibitem{okada2}
岡田眞也, 清岡優祐, 春山健太, 上田隆一, 林原靖男, “視覚と
行動の end-to-end 学習により経路追従行動 をオンラインで
模倣する手法の提案 -経路追従行動の修正のためにデータセッ
トを動的に追加する手法の検討-”, 計測自動制御学会 SI 部門
講演会 SICE-SI2021 予稿集, pp.1066-1080, 2021.

\bibitem{kiyooka}
清岡優祐, 岡田眞也, 岩井一輝, 上田隆一, 林原靖男, “視覚と行動の end-
to-end 学習により経路追従行動 をオンラインで模倣する手法の提案 -データセットと
生成された経路追従行動の解析-”, 計測自動 制御学会 SI 部門講演会 SICE-SI2021 
予稿集, pp.1071-1075, 2021.

\bibitem{off_load}
U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. Cun.
“Off-Road Obstacle Avoidance through End-to-End Learning.”
Advances in neural information processing systems,
Vol. 18, 2005.

\bibitem{Moridian}
Moridian, Barzin, Anurag Kamal, and Nina Mahmoudian. 
“Learning Navigation Tasks from Demonstration for Semi-autonomous Remote
Operation of Mobile Robots.” 2018 IEEE International Symposium on
Safety, Security, and Rescue Robotics (SSRR), pp.1-8, 2018.

\bibitem{Bojarski}
Bojarski, Mariusz, \textit{et al}.,
“End to end learning for self-driving cars,” arXiv:1604.08316, 2016.

\bibitem{imai}
今井悠月 , 清岡優祐 , 春山健太 , 上田隆一 , 林原靖男 . ” 視覚と行動の end-to-end 学習によ
り経路追従行動をオンラインで模倣する手法の提案
ー経路への復帰行動の解析と復帰
行動を強化する教師データ収集法の検討ー . 日本機械学会ロボティクス・メカトロニクス
講演会 ’23 予稿集 , 2P2-G05(2023).

\bibitem{fuji}
藤原柾，春山健太，馬場琉生，上田隆一，林原靖男，"視覚と行動のend-to-end 学習により経路追従行動を
オンラインで模倣する手法の提案　実環境における経路選択機能の検証と学習時間の短縮化の検討"
，日本機械学会ロボティクス・メカトロニクス講演会'23予稿集，2P2-G06(2023)


\bibitem{takahashi}
\CID{8705}橋祐樹，白須和暉，藤原柾，上田隆一，林原靖男，"視覚と行動のend-to-end 学習による経路追従行動の模倣"，
日本機械学会ロボティクス・メカトロニクス講演会'23予稿集，2P1-G07(2023)

\bibitem{imai2}
今井 悠月、落合 拓海、白須 和暉、上田 隆一、林原 靖男，”視覚と行動のend-to-end 学習により経路追従行動を
オンラインで模倣する手法の提案 モデルの評価に基づく訓練ステップ数の決定”，1B1-04，SI2023，(2023)


\end{thebibliography}

\normalsize
\end{document}
